I am part of the Bachelor’s of Science program (BSc) and the reports will be written in English. I will be using python in this project, and python is the only language I feel proficient enough in for peer reviewing other projects in. 

The core of this project is Artificial Intelligence, more specifically, Neural Networks. I want to implement a simple neural network with only a few layers, which using a backpropagation algorithm is able to learn to recognize which number any handwritten digit represents in the MNIST dataset. 

The algorithms I will use are the Feed-Forward algorithm, which computes the output given an input by feeding it through all of the neurons and then finally an output layer which can for example be the softmax function. Then we have the Back Propagation algorithm, which computes the gradient of the loss function with respect to all the neuron weights using the chain rule starting from the output layer and moving backwards. The gradients are then used in SGD to update all the weights to something more optimal. Lastly we have Stochastic Gradient Descent (SGD) which is used to update the model parameter to minimize loss in mini-batches of the data. The data structures needed for this include 1D and 2D arrays for e.g storing the weights between neurons, dictionaries and/or lists to organize parameters/gradients. 

During training, the program will receive a grayscale image of a handwritten digit (from 0-9) and its label. It is then fed forward through the neural network, which will produce an output prediction of which integer is written in the image. The loss is computed and then differentiated with respect to each weight and bias during back propagation, and then finally the model hyperparameters (weights and biases) are updated with these gradients using SGD to achieve a minimal loss. In the end, after training, the model should be able to predict the number in the image as an integer from 0-9. 

The big O analysis for these algorithms is rather quite simple. For the feed forward, a single  input (I) needs to pass through every layer in the network and every neuron (N) in each layer, hence the time complexity is O(N I), where N is the number of total neurons and I is the number of inputs. The back propagation is essentially the same but just backwards so it also has a time complexity of O(N I). The SGD passes through every weight (W) to update it, hence has a time complexity of O(W) where W is the number of weights. All of these are for a single iteration. 

The sources I intend to use are Michael A. Nielsen’s book “Neural Networks and Deep Learning”, the Medium article “A step-by-step forward pass and backpropagation example” by Rabindra Lamsal, and the youtube channel 3Blue1Brown
