This week I changed the absolute file path to work on every device in mnist_loader.py as pointed out in the peer review feedback. I also improved code quality by fixing pylint errors. I changed weight initialization from standard gaussian to xavier as suggested in the peer review, since it works better for preventing vanishing or exploding gradients, which sigmoid is prone to (following this source https://www.geeksforgeeks.org/deep-learning/xavier-initialization/). I decided not to change the activation function or the cost function (as was suggested in peer feedback)  since improving/optimizing model performance is not within the scope of the project. The main goal and scope was to learn how each stage of a neural net works by implementing a simple one, which is already achieved by my current selection of activation and cost functions.  
I also created and added the equations_explained.markdown file as well as the README.md file. The project has progressed well and nothing remains unclear.
